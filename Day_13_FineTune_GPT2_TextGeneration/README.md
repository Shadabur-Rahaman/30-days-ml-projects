# ğŸ¤– Day 13 â€“ Fine-Tuning GPT-2 for Text Generation

In this project, I fine-tuned OpenAIâ€™s GPT-2 model on a custom dataset using the HuggingFace Transformers library. The fine-tuned model can generate coherent and stylistically consistent text based on the given corpus.

![GPT-2 Text Generation](images/gpt2_textgen_sample.png)

## ğŸ” Overview

- Model: GPT-2 (124M parameters)
- Dataset: Custom text corpus
- Library: HuggingFace Transformers
- Tasks: Text preprocessing, tokenization, fine-tuning, generation
```bash
## ğŸ“ File Structure

Day13_FineTune_GPT2_TextGeneration/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ custom_corpus.txt
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ Day13_FineTune_GPT2_TextGeneration.ipynb
â”œâ”€â”€ outputs/
â”‚ â”œâ”€â”€ generated_text_samples.txt
â”‚ â””â”€â”€ model_checkpoints/
â”œâ”€â”€ src/
â”‚ â””â”€â”€ gpt2_finetune_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


## âš™ï¸ How to Run

pip install -r requirements.txt
Then open and run:


notebooks/Day13_FineTune_GPT2_TextGeneration.ipynb
Make sure your custom_corpus.txt is placed under data/.

ğŸ“š Learning Outcomes
Understand how transformers learn language

Apply transfer learning using GPT-2

Fine-tune a pre-trained language model on custom data

Generate creative, contextual text outputs

