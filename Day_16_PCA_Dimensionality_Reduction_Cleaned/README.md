# Principal Component Analysis (PCA) â€“ Dimensionality Reduction

This project demonstrates how to apply **PCA** to reduce high-dimensional data into lower dimensions while preserving the most important features.

![PCA Projection](images/pca_2d_projection.png)

## ğŸ“Œ Overview

- **Dataset:** Iris dataset (or any high-dimensional dataset)
- **Goal:** Reduce features from N-dim to 2D/3D for visualization
- **Techniques:** Standardization, PCA, Scree Plot, 2D/3D Visualization
- **Libraries:** Scikit-learn, matplotlib, seaborn

## ğŸ“ Directory Structure
```bash
Day16_PCA_Dimensionality_Reduction_Cleaned/
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ Day_16_PCA_Dimensionality_Reduction_Cleaned.ipynb
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ explained_variance_ratio.png
â”‚ â”œâ”€â”€ pca_2d_projection.png
â”‚ â””â”€â”€ pca_3d_projection.png
â”œâ”€â”€ src/
â”‚ â””â”€â”€ pca_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


## ğŸš€ How to Run


pip install -r requirements.txt
jupyter notebook notebooks/Day_16_PCA_Dimensionality_Reduction_Cleaned.ipynb

ğŸ¯ Learning Outcomes
Understand dimensionality reduction and the PCA algorithm

Visualize high-dimensional data in 2D and 3D space

Evaluate variance retained using PCA

ğŸ“Œ This is Day 16 of my #30DaysMLProjects journey.

