# 🤖 Chatbot using Seq2Seq (Day 20 - #30DaysMLProjects)

This project implements a basic **chatbot** using a **Sequence-to-Sequence (Seq2Seq)** model with an **Encoder-Decoder LSTM** architecture. It demonstrates how deep learning can be used for conversational AI.

---

## 📌 Features
- Text preprocessing (cleaning, tokenization, padding)
- Seq2Seq architecture using LSTMs
- Teacher forcing for better training
- Response generation using greedy decoding

---

## 📁 Project Structure
```bash
Day20_Chatbot_Seq2Seq/
├── notebooks/
│   └── Day_20_Chatbot_Seq2Seq_Model.ipynb   # Cleaned final notebook
├── src/
│   ├── data_preprocessing.py                # Tokenization, padding, cleaning
│   ├── model_seq2seq.py                     # Encoder-Decoder LSTM architecture
│   ├── inference.py                         # Code to generate responses
├── outputs/
│   └── sample_conversations.txt             # Example chatbot conversations
│   └── training_plot.png                    # Loss vs Epoch plot
├── README.md
├── requirements.txt
└── .gitignore


---

## 🚀 How to Run

1. Clone the repo:

git clone https://github.com/YourUsername/30-days-ml-projects.git
cd Day20_Chatbot_Seq2Seq
Install dependencies:

pip install -r requirements.txt
Run training script (or use Jupyter notebook):


python src/model_seq2seq.py
Chat with your bot:


python src/inference.py
🛠 Libraries Used
TensorFlow / Keras

Numpy

NLTK

Matplotlib

📦 Output
sample_conversations.txt: Responses generated by chatbot

training_plot.png: Training loss visualization

📚 References
Seq2Seq with Keras (Chatbot example)

TensorFlow Encoder-Decoder Tutorials

📌 Coming Up Next
Day 21: End-to-End Image Captioning using CNN + RNN
Stay tuned! 🚀
