{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadabur-Rahaman/30-days-ml-projects/blob/main/Day_30_End2End-ML-Pipeline/notebook/30_end_to_end_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8117d011",
      "metadata": {
        "id": "8117d011"
      },
      "source": [
        "# End-to-End ML Pipeline: Iris Classification\n",
        "## Day 30/30 of Machine Learning Project\n",
        "\n",
        "**Project Goal**: Implement a complete ML pipeline from data collection to deployment\n",
        "\n",
        "**Pipeline Stages**:\n",
        "1. Data Collection & Preprocessing\n",
        "2. Model Training & Evaluation\n",
        "3. Model Packaging\n",
        "4. API Development\n",
        "5. Containerization\n",
        "6. Deployment\n",
        "7. Monitoring\n",
        "\n",
        "**Technologies Used**:\n",
        "- Scikit-learn for ML\n",
        "- FastAPI for REST API\n",
        "- Docker for containerization\n",
        "- MLflow for experiment tracking\n",
        "- Grafana for monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a9c835",
      "metadata": {
        "id": "f9a9c835"
      },
      "source": [
        "## 1. Data Collection & Preprocessing\n",
        "Collect and prepare the Iris dataset for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7367d09a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "7367d09a",
        "outputId": "7c52be14-4f33-4f96-bda2-ddcebcd9b664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 150 samples\n",
            "Training samples: 120\n",
            "Testing samples: 30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target species  \n",
              "0       0  setosa  \n",
              "1       0  setosa  \n",
              "2       0  setosa  \n",
              "3       0  setosa  \n",
              "4       0  setosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-174c0285-38a2-4aa3-b56e-5a135d685f8e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-174c0285-38a2-4aa3-b56e-5a135d685f8e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-174c0285-38a2-4aa3-b56e-5a135d685f8e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-174c0285-38a2-4aa3-b56e-5a135d685f8e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-77855ef2-d5fd-4cd0-a93e-40da3caf59c9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77855ef2-d5fd-4cd0-a93e-40da3caf59c9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-77855ef2-d5fd-4cd0-a93e-40da3caf59c9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"sepal length (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal width (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.435866284936698,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal length (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7652982332594667,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal width (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7622376689603465,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"setosa\",\n          \"versicolor\",\n          \"virginica\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "df['species'] = df['target'].apply(lambda x: iris.target_names[x])\n",
        "\n",
        "# Save raw data\n",
        "df.to_csv('data/iris_raw.csv', index=False)\n",
        "\n",
        "# Split data\n",
        "X = df[iris.feature_names]\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save processed data\n",
        "X_train.to_csv('data/X_train.csv', index=False)\n",
        "X_test.to_csv('data/X_test.csv', index=False)\n",
        "y_train.to_csv('data/y_train.csv', index=False)\n",
        "y_test.to_csv('data/y_test.csv', index=False)\n",
        "\n",
        "print(f\"Dataset size: {len(df)} samples\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ac6196",
      "metadata": {
        "id": "c0ac6196"
      },
      "source": [
        "## 2. Model Training & Evaluation\n",
        "Train multiple models and track experiments with MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "96863e73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96863e73",
        "outputId": "ac5eb7eb-edb0-41c4-f438-068f58912acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/06/13 19:04:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "\u001b[31m2025/06/13 19:04:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
            "2025/06/13 19:04:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression - Accuracy: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/06/13 19:04:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM - Accuracy: 1.0000, F1: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/06/13 19:04:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "\u001b[31m2025/06/13 19:04:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Accuracy: 1.0000, F1: 1.0000\n",
            "\n",
            "Best model saved: LogisticRegression with accuracy 1.0000\n"
          ]
        }
      ],
      "source": [
        "!pip install mlflow scikit-learn -q\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up MLflow\n",
        "mlflow.set_tracking_uri('file:./mlruns')\n",
        "mlflow.set_experiment('Iris-Classification')\n",
        "\n",
        "# Load data\n",
        "X_train = pd.read_csv('data/X_train.csv')\n",
        "X_test = pd.read_csv('data/X_test.csv')\n",
        "y_train = pd.read_csv('data/y_train.csv').squeeze()\n",
        "y_test = pd.read_csv('data/y_test.csv').squeeze()\n",
        "\n",
        "# Define models to evaluate\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
        "    'SVM': SVC(probability=True),\n",
        "    'Random Forest': RandomForestClassifier()\n",
        "}\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # Log parameters and metrics\n",
        "        mlflow.log_param('model', model_name)\n",
        "        mlflow.log_metric('accuracy', accuracy)\n",
        "        mlflow.log_metric('f1_score', f1)\n",
        "\n",
        "        # Log model\n",
        "        mlflow.sklearn.log_model(model, 'model')\n",
        "\n",
        "        # Save confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=iris.target_names,\n",
        "                   yticklabels=iris.target_names)\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.savefig(f'monitoring/cm_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "        mlflow.log_artifact(f'monitoring/cm_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Track best model\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "\n",
        "        print(f\"{model_name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Save best model\n",
        "import joblib\n",
        "joblib.dump(best_model, 'models/iris_classifier.joblib')\n",
        "print(f\"\\nBest model saved: {type(best_model).__name__} with accuracy {best_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1be08c",
      "metadata": {
        "id": "6e1be08c"
      },
      "source": [
        "## 3. Model Packaging\n",
        "Package the model with preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f4b2b4b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4b2b4b3",
        "outputId": "cf79610b-03c4-4f7f-f654-6d4efa4598d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample prediction: versicolor\n",
            "Probabilities: {np.str_('setosa'): np.float64(0.009627502570247233), np.str_('versicolor'): np.float64(0.8995825425389817), np.str_('virginica'): np.float64(0.0907899548907711)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = StandardScaler()\n",
        "\n",
        "# Create full pipeline\n",
        "final_model = Pipeline([\n",
        "    ('scaler', preprocessor),\n",
        "    ('classifier', best_model)\n",
        "])\n",
        "\n",
        "# Retrain on full data\n",
        "X_full = pd.concat([X_train, X_test])\n",
        "y_full = pd.concat([y_train, y_test])\n",
        "final_model.fit(X_full, y_full)\n",
        "\n",
        "# Save final model\n",
        "joblib.dump(final_model, 'models/iris_pipeline.joblib')\n",
        "\n",
        "# Test prediction\n",
        "sample = X_test.iloc[0].values.reshape(1, -1)\n",
        "pred = final_model.predict(sample)\n",
        "prob = final_model.predict_proba(sample)\n",
        "print(f\"Sample prediction: {iris.target_names[pred[0]]}\")\n",
        "print(f\"Probabilities: {dict(zip(iris.target_names, prob[0]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa067d6",
      "metadata": {
        "id": "9aa067d6"
      },
      "source": [
        "## 4. API Development with FastAPI\n",
        "Create a REST API for model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e44b1c3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44b1c3b",
        "outputId": "77210bb5-7533-4b74-b30b-659f58c3b60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing api/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile api/main.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load model\n",
        "model_path = os.path.join(os.path.dirname(__file__), 'models/iris_pipeline.joblib')\n",
        "model = joblib.load(model_path)\n",
        "\n",
        "# Define class names\n",
        "class_names = ['setosa', 'versicolor', 'virginica']\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"Iris Classification API\")\n",
        "\n",
        "# Define request model\n",
        "class IrisFeatures(BaseModel):\n",
        "    sepal_length: float\n",
        "    sepal_width: float\n",
        "    petal_length: float\n",
        "    petal_width: float\n",
        "\n",
        "# Define response model\n",
        "class PredictionResult(BaseModel):\n",
        "    species: str\n",
        "    confidence: float\n",
        "    probabilities: dict\n",
        "\n",
        "@app.get('/')\n",
        "def health_check():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "@app.post('/predict', response_model=PredictionResult)\n",
        "def predict(features: IrisFeatures):\n",
        "    \"\"\"Make prediction for Iris flower\"\"\"\n",
        "    # Convert features to DataFrame\n",
        "    input_data = pd.DataFrame([features.dict()])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(input_data)\n",
        "    probabilities = model.predict_proba(input_data)[0]\n",
        "\n",
        "    # Get confidence\n",
        "    confidence = np.max(probabilities)\n",
        "\n",
        "    # Format probabilities\n",
        "    prob_dict = {class_names[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
        "\n",
        "    return {\n",
        "        \"species\": class_names[prediction[0]],\n",
        "        \"confidence\": confidence,\n",
        "        \"probabilities\": prob_dict\n",
        "    }\n",
        "\n",
        "@app.get('/model_info')\n",
        "def model_info():\n",
        "    \"\"\"Get model information\"\"\"\n",
        "    return {\n",
        "        \"model_type\": type(model.named_steps['classifier']).__name__,\n",
        "        \"features\": model.named_steps['classifier'].feature_importances_.tolist() if hasattr(model.named_steps['classifier'], 'feature_importances_') else \"Not available\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d34f445d",
      "metadata": {
        "id": "d34f445d"
      },
      "source": [
        "## 5. Containerization with Docker\n",
        "Package the API in a Docker container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7ed410dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ed410dc",
        "outputId": "95016039-80ef-463d-9b26-965b679cabf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing api/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile api/Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy application files\n",
        "COPY . .\n",
        "\n",
        "# Expose port\n",
        "EXPOSE 8000\n",
        "\n",
        "# Command to run the application\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9727da73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9727da73",
        "outputId": "9fcd42e9-971c-4f3e-b0ce-60bcae656449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing api/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile api/requirements.txt\n",
        "fastapi\n",
        "uvicorn\n",
        "scikit-learn\n",
        "pandas\n",
        "numpy\n",
        "pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac208c90",
      "metadata": {
        "id": "ac208c90"
      },
      "source": [
        "## 6. Deployment\n",
        "Build and run the Docker container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "de283470",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de283470",
        "outputId": "03206188-f153-413c-de44-c25f058a30d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API not running. Starting API...\n",
            "API not running. Starting API...\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install fastapi uvicorn python-multipart -q\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Start API in background\n",
        "api_process = subprocess.Popen(\n",
        "    [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n",
        "    cwd=os.getcwd()\n",
        ")\n",
        "\n",
        "# Wait for API to start\n",
        "time.sleep(3)\n",
        "\n",
        "# Test API\n",
        "def test_api():\n",
        "    try:\n",
        "        # Health check\n",
        "        health_response = requests.get(\"http://localhost:8000/\")\n",
        "        print(f\"Health Check Status: {health_response.status_code}\")\n",
        "        print(f\"Response: {health_response.json()}\")\n",
        "\n",
        "        # Test prediction\n",
        "        sample_data = {\n",
        "            \"sepal_length\": 5.1,\n",
        "            \"sepal_width\": 3.5,\n",
        "            \"petal_length\": 1.4,\n",
        "            \"petal_width\": 0.2\n",
        "        }\n",
        "        pred_response = requests.post(\"http://localhost:8000/predict\", json=sample_data)\n",
        "        print(f\"\\nPrediction Status: {pred_response.status_code}\")\n",
        "        print(\"Prediction Response:\")\n",
        "        print(pred_response.json())\n",
        "\n",
        "        # Model info\n",
        "        model_info = requests.get(\"http://localhost:8000/model_info\")\n",
        "        print(f\"\\nModel Info Status: {model_info.status_code}\")\n",
        "        print(\"Model Info:\")\n",
        "        print(model_info.json())\n",
        "\n",
        "        return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"API not running. Starting API...\")\n",
        "        return False\n",
        "\n",
        "# Test API - retry if needed\n",
        "if not test_api():\n",
        "    # Start API if not running\n",
        "    api_process = subprocess.Popen(\n",
        "        [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n",
        "        cwd=os.getcwd()\n",
        "    )\n",
        "    time.sleep(3)\n",
        "    test_api()\n",
        "\n",
        "# Stop API after testing\n",
        "api_process.terminate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53122f1c",
      "metadata": {
        "id": "53122f1c"
      },
      "source": [
        "## 7. Monitoring & Logging\n",
        "Set up basic monitoring with Prometheus and Grafana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1eea04bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eea04bc",
        "outputId": "9fe9f0f8-80fb-4bce-8bdd-5d61fb07cd21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting api/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile api/main.py\n",
        "# ... previous API code ...\n",
        "\n",
        "# Add logging\n",
        "import logging\n",
        "import time\n",
        "\n",
        "logging.basicConfig(filename='api_logs.log', level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "@app.middleware(\"http\")\n",
        "async def log_requests(request: Request, call_next):\n",
        "    start_time = time.time()\n",
        "    response = await call_next(request)\n",
        "    process_time = (time.time() - start_time) * 1000\n",
        "    formatted_time = f\"{process_time:.2f}ms\"\n",
        "\n",
        "    logging.info(\n",
        "        f\"{request.method} {request.url} - \"\n",
        "        f\"Status: {response.status_code} - \"\n",
        "        f\"Time: {formatted_time}\"\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Add metrics endpoint\n",
        "from prometheus_client import make_asgi_app, Counter, Histogram\n",
        "\n",
        "# Create metrics\n",
        "REQUEST_COUNT = Counter(\n",
        "    'api_request_count',\n",
        "    'API Request Count',\n",
        "    ['method', 'endpoint', 'status']\n",
        ")\n",
        "\n",
        "REQUEST_LATENCY = Histogram(\n",
        "    'api_request_latency_seconds',\n",
        "    'API Request Latency',\n",
        "    ['method', 'endpoint']\n",
        ")\n",
        "\n",
        "# Add Prometheus metrics route\n",
        "metrics_app = make_asgi_app()\n",
        "app.mount(\"/metrics\", metrics_app)\n",
        "\n",
        "@app.middleware(\"http\")\n",
        "async def monitor_requests(request: Request, call_next):\n",
        "    start_time = time.time()\n",
        "    response = await call_next(request)\n",
        "    process_time = time.time() - start_time\n",
        "\n",
        "    REQUEST_COUNT.labels(\n",
        "        method=request.method,\n",
        "        endpoint=request.url.path,\n",
        "        status=response.status_code\n",
        "    ).inc()\n",
        "\n",
        "    REQUEST_LATENCY.labels(\n",
        "        method=request.method,\n",
        "        endpoint=request.url.path\n",
        "    ).observe(process_time)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c2d23737",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2d23737",
        "outputId": "94d227b3-d7a8-4e51-c3a6-d60b879419a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing monitoring/prometheus.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile monitoring/prometheus.yml\n",
        "global:\n",
        "  scrape_interval: 15s\n",
        "\n",
        "scrape_configs:\n",
        "  - job_name: 'iris-api'\n",
        "    static_configs:\n",
        "      - targets: ['host.docker.internal:8000']  # For Mac/Windows\n",
        "        # Use 'docker.for.mac.localhost' for older Docker Mac versions\n",
        "        # Use 'docker.for.win.localhost' for Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "eeca2da9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeca2da9",
        "outputId": "2f433b3d-3ab5-4792-bed5-50d026a3bd1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: docker-compose: command not found\n",
            "Monitoring services started:\n",
            "- Prometheus: http://localhost:9090\n",
            "- Grafana: http://localhost:3000 (admin/admin)\n",
            "\n",
            "Configure Grafana dashboard with Prometheus as data source\n"
          ]
        }
      ],
      "source": [
        "# Start monitoring stack\n",
        "!docker-compose -f monitoring/docker-compose.yml up -d\n",
        "\n",
        "print(\"Monitoring services started:\")\n",
        "print(\"- Prometheus: http://localhost:9090\")\n",
        "print(\"- Grafana: http://localhost:3000 (admin/admin)\")\n",
        "print(\"\\nConfigure Grafana dashboard with Prometheus as data source\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b178cb40",
      "metadata": {
        "id": "b178cb40"
      },
      "source": [
        "## 8. CI/CD Pipeline\n",
        "Example GitHub Actions workflow for automated testing and deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b5a78abb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5a78abb",
        "outputId": "12338e2a-4bc4-4d47-d907-ffe88e5c681e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing github/workflows/ml-pipeline.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile github/workflows/ml-pipeline.yml\n",
        "name: ML Pipeline\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [ main ]\n",
        "  pull_request:\n",
        "    branches: [ main ]\n",
        "\n",
        "jobs:\n",
        "  test:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "\n",
        "    - name: Set up Python\n",
        "      uses: actions/setup-python@v4\n",
        "      with:\n",
        "        python-version: '3.9'\n",
        "\n",
        "    - name: Install dependencies\n",
        "      run: |\n",
        "        python -m pip install --upgrade pip\n",
        "        pip install -r api/requirements.txt\n",
        "        pip install pytest\n",
        "\n",
        "    - name: Run tests\n",
        "      run: |\n",
        "        pytest tests/\n",
        "\n",
        "  build-and-deploy:\n",
        "    needs: test\n",
        "    runs-on: ubuntu-latest\n",
        "    if: github.ref == 'refs/heads/main'\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "\n",
        "    - name: Build Docker image\n",
        "      run: docker build -t iris-api api/\n",
        "\n",
        "    - name: Deploy to AWS ECS\n",
        "      uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n",
        "      with:\n",
        "        task-definition: task-definition.json\n",
        "        service: iris-service\n",
        "        cluster: iris-cluster\n",
        "        wait-for-service-stability: true"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73247c37",
      "metadata": {
        "id": "73247c37"
      },
      "source": [
        "## 9. Testing\n",
        "Create unit tests for the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "dec415c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dec415c8",
        "outputId": "24a87c87-e1a8-4590-b7ff-97a63eeb3606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_api.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_api.py\n",
        "import pytest\n",
        "from fastapi.testclient import TestClient\n",
        "from api.main import app\n",
        "\n",
        "client = TestClient(app)\n",
        "\n",
        "def test_health_check():\n",
        "    response = client.get(\"/\")\n",
        "    assert response.status_code == 200\n",
        "    assert response.json() == {\"status\": \"healthy\"}\n",
        "\n",
        "def test_predict():\n",
        "    sample = {\n",
        "        \"sepal_length\": 5.1,\n",
        "        \"sepal_width\": 3.5,\n",
        "        \"petal_length\": 1.4,\n",
        "        \"petal_width\": 0.2\n",
        "    }\n",
        "    response = client.post(\"/predict\", json=sample)\n",
        "    assert response.status_code == 200\n",
        "    data = response.json()\n",
        "    assert \"species\" in data\n",
        "    assert \"confidence\" in data\n",
        "    assert \"probabilities\" in data\n",
        "    assert data[\"species\"] in [\"setosa\", \"versicolor\", \"virginica\"]\n",
        "\n",
        "def test_model_info():\n",
        "    response = client.get(\"/model_info\")\n",
        "    assert response.status_code == 200\n",
        "    data = response.json()\n",
        "    assert \"model_type\" in data\n",
        "    assert \"features\" in data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from prometheus_client import make_asgi_app, Counter, Histogram\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data and train model\n",
        "iris = load_iris()\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
        "])\n",
        "model.fit(iris.data, iris.target)\n",
        "\n",
        "# Save and load model\n",
        "joblib.dump(model, 'iris_model.joblib')\n",
        "model = joblib.load('iris_model.joblib')\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"Iris Classification API\")\n",
        "\n",
        "# Data for visualizations\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['species'] = iris.target_names[iris.target]\n",
        "\n",
        "# Visualization Functions\n",
        "def create_feature_plot():\n",
        "    \"\"\"Create feature distribution plot\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(data=df.melt(id_vars='species'), x='variable', y='value', hue='species')\n",
        "    plt.title('Feature Distributions by Species')\n",
        "    plt.ylabel('Measurement (cm)')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.legend(title='Species')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save to buffer\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "def create_decision_boundary_plot():\n",
        "    \"\"\"Create PCA decision boundary plot\"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "    from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "    # Reduce to 2D with PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(iris.data)\n",
        "\n",
        "    # Train a simple classifier for visualization\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_pca, iris.target)\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plot_decision_regions(X_pca, iris.target, clf=clf, legend=2)\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.title('Decision Boundaries (PCA Reduced)')\n",
        "\n",
        "    # Save to buffer\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "# API Endpoints with Visualizations\n",
        "@app.get('/feature_plot')\n",
        "def get_feature_plot():\n",
        "    \"\"\"Endpoint to get feature distribution plot\"\"\"\n",
        "    try:\n",
        "        plot_data = create_feature_plot()\n",
        "        return {\"plot\": plot_data}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get('/decision_plot')\n",
        "def get_decision_plot():\n",
        "    \"\"\"Endpoint to get decision boundary plot\"\"\"\n",
        "    try:\n",
        "        plot_data = create_decision_boundary_plot()\n",
        "        return {\"plot\": plot_data}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# ... (rest of the API code from previous implementation) ..."
      ],
      "metadata": {
        "id": "NgXurCwIqQo1"
      },
      "id": "NgXurCwIqQo1",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frontend.py (updated)\n",
        "import gradio as gr\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Configuration\n",
        "API_URL = \"http://127.0.0.1:8000/predict\"  # Changed from localhost\n",
        "TIMEOUT = 5  # seconds\n",
        "iris = load_iris()\n",
        "\n",
        "def safe_api_call(url, json_data=None):\n",
        "    \"\"\"Generic API call with error handling\"\"\"\n",
        "    try:\n",
        "        if json_data:\n",
        "            response = requests.post(url, json=json_data, timeout=TIMEOUT)\n",
        "        else:\n",
        "            response = requests.get(url, timeout=TIMEOUT)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        return {\"error\": f\"API returned {response.status_code}\"}\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return {\"error\": f\"Could not connect to API at {url}. Is the server running?\"}\n",
        "    except requests.exceptions.Timeout:\n",
        "        return {\"error\": \"API request timed out\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Unexpected error: {str(e)}\"}\n",
        "\n",
        "def classify_flower(sepal_length, sepal_width, petal_length, petal_width):\n",
        "    \"\"\"Enhanced classification function with visualization\"\"\"\n",
        "    data = {\n",
        "        \"sepal_length\": float(sepal_length),\n",
        "        \"sepal_width\": float(sepal_width),\n",
        "        \"petal_length\": float(petal_length),\n",
        "        \"petal_width\": float(petal_width)\n",
        "    }\n",
        "\n",
        "    # API call\n",
        "    result = safe_api_call(API_URL, data)\n",
        "\n",
        "    if \"error\" in result:\n",
        "        return result[\"error\"], \"\", \"\", None, None\n",
        "\n",
        "    # Visualization functions\n",
        "    def create_prob_plot(probs):\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.bar(probs.keys(), probs.values(), color=['#ff9999','#66b3ff','#99ff99'])\n",
        "        plt.title(\"Class Probabilities\")\n",
        "        plt.ylim(0, 1)\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return buf\n",
        "\n",
        "    def create_feature_plot(input_features, species_avg):\n",
        "        features = list(input_features.keys())\n",
        "        values = list(input_features.values())\n",
        "        avg_values = species_avg.loc[species_avg['species'] == result['species']][features].values[0]\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        x = range(len(features))\n",
        "        plt.bar(x, values, width=0.4, label='Your Input')\n",
        "        plt.bar([i + 0.4 for i in x], avg_values, width=0.4, label='Species Average')\n",
        "        plt.xticks([i + 0.2 for i in x], features)\n",
        "        plt.legend()\n",
        "        plt.title(\"Feature Comparison\")\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return buf\n",
        "\n",
        "    # Prepare species averages\n",
        "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "    df['species'] = iris.target_names[iris.target]\n",
        "    species_avg = df.groupby('species').mean().reset_index()\n",
        "\n",
        "    return (\n",
        "        f\"Predicted: {result['species'].upper()}\",\n",
        "        f\"Confidence: {result['confidence']:.1%}\",\n",
        "        \"\\n\".join([f\"{k}: {v:.1%}\" for k, v in result['probabilities'].items()]),\n",
        "        create_prob_plot(result['probabilities']),\n",
        "        create_feature_plot(data, species_avg)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "lj7NazEHtGEL"
      },
      "id": "lj7NazEHtGEL",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "# Configure API URL\n",
        "if IN_COLAB:\n",
        "    from google.colab import output\n",
        "    API_PORT = 8000\n",
        "    output.serve_kernel_port_as_iframe(API_PORT)\n",
        "    API_URL = f\"https://localhost:{API_PORT}/predict\"\n",
        "else:\n",
        "    API_URL = \"http://127.0.0.1:8000/predict\"\n",
        "\n",
        "def classify_flower(sepal_length, sepal_width, petal_length, petal_width):\n",
        "    try:\n",
        "        response = requests.post(API_URL, json={\n",
        "            \"sepal_length\": sepal_length,\n",
        "            \"sepal_width\": sepal_width,\n",
        "            \"petal_length\": petal_length,\n",
        "            \"petal_width\": petal_width\n",
        "        }, timeout=5)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            return f\"Predicted: {result['species']}\", f\"Confidence: {result['confidence']:.0%}\"\n",
        "        return \"API Error\", f\"Status code: {response.status_code}\"\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        if IN_COLAB:\n",
        "            return \"Connection Error\", \"Ensure you ran the API cell first\"\n",
        "        return \"Connection Error\", \"Start API with: uvicorn api.main:app --port 8000\"\n",
        "    except Exception as e:\n",
        "        return \"Error\", str(e)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Iris Classifier\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            sl = gr.Slider(4, 8, value=5.1, label=\"Sepal Length\")\n",
        "            sw = gr.Slider(2, 5, value=3.5, label=\"Sepal Width\")\n",
        "            pl = gr.Slider(1, 7, value=1.4, label=\"Petal Length\")\n",
        "            pw = gr.Slider(0.1, 3, value=0.2, label=\"Petal Width\")\n",
        "            btn = gr.Button(\"Classify\")\n",
        "        with gr.Column():\n",
        "            species = gr.Textbox(label=\"Prediction\")\n",
        "            confidence = gr.Textbox(label=\"Confidence\")\n",
        "\n",
        "    btn.click(classify_flower, [sl, sw, pl, pw], [species, confidence])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if IN_COLAB:\n",
        "        demo.launch(share=True, server_port=7860)\n",
        "    else:\n",
        "        demo.launch(server_port=7860)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "EZsItr7E2Lyq",
        "outputId": "dcc500e8-a726-4280-cf6d-bac04f6a322b"
      },
      "id": "EZsItr7E2Lyq",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8000, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-2579055901>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7860\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_port\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7860\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2722\u001b[0m                     \u001b[0mlocal_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m                     \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m                     \u001b[0mapp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m                     \u001b[0mserver_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         raise OSError(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;34mf\"Cannot find empty port in range: {min(server_ports)}-{max(server_ports)}. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install only Gradio - no need for FastAPI or ngrok\n",
        "!pip install gradio scikit-learn > /dev/null\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load iris dataset and train a model\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create classification function\n",
        "def classify_flower(sepal_length, sepal_width, petal_length, petal_width):\n",
        "    # Create input array\n",
        "    input_data = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(input_data)[0]\n",
        "    probabilities = model.predict_proba(input_data)[0]\n",
        "\n",
        "    # Get species name\n",
        "    species = iris.target_names[prediction]\n",
        "\n",
        "    # Format results\n",
        "    confidence = probabilities[prediction]\n",
        "    prob_text = \"\\n\".join([f\"{iris.target_names[i]}: {prob:.1%}\" for i, prob in enumerate(probabilities)])\n",
        "\n",
        "    # Create visualization\n",
        "    plt = create_visualization(probabilities, species, [sepal_length, sepal_width, petal_length, petal_width])\n",
        "\n",
        "    return species, f\"{confidence:.1%}\", prob_text, plt\n",
        "\n",
        "# Create visualization\n",
        "def create_visualization(probabilities, species, features):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Probability plot\n",
        "    colors = ['#FF9999', '#66B3FF', '#99FF99']\n",
        "    bars = ax1.bar(iris.target_names, probabilities, color=colors)\n",
        "    ax1.set_title('Class Probabilities', fontweight='bold')\n",
        "    ax1.set_ylabel('Probability')\n",
        "    ax1.set_ylim(0, 1)\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1%}', ha='center', va='bottom')\n",
        "\n",
        "    # Feature comparison radar plot\n",
        "    categories = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
        "    N = len(categories)\n",
        "\n",
        "    # Compute angles\n",
        "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "\n",
        "    # Species averages\n",
        "    species_avg = []\n",
        "    for i in range(len(iris.target_names)):\n",
        "        species_data = iris.data[iris.target == i]\n",
        "        species_avg.append(np.mean(species_data, axis=0))\n",
        "\n",
        "    # Plot setup\n",
        "    ax2 = plt.subplot(122, polar=True)\n",
        "    ax2.set_theta_offset(np.pi/2)\n",
        "    ax2.set_theta_direction(-1)\n",
        "\n",
        "    # Plot species averages\n",
        "    for i, species_name in enumerate(iris.target_names):\n",
        "        values = species_avg[i].tolist()\n",
        "        values += values[:1]\n",
        "        ax2.plot(angles, values, linewidth=1, linestyle='solid', label=species_name)\n",
        "        ax2.fill(angles, values, alpha=0.1)\n",
        "\n",
        "    # Plot input features\n",
        "    input_values = features + [features[0]]\n",
        "    ax2.plot(angles, input_values, color='red', linewidth=2, linestyle='solid', label='Your Input')\n",
        "    ax2.scatter(angles, input_values, color='red', s=50)\n",
        "\n",
        "    # Add labels\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "    plt.title('Feature Comparison', fontweight='bold')\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"Iris Flower Classifier\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# 🌸 Iris Flower Classifier\")\n",
        "    gr.Markdown(\"Enter measurements to classify iris flowers\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            sl = gr.Slider(4.0, 8.0, value=5.1, step=0.1, label=\"Sepal Length (cm)\")\n",
        "            sw = gr.Slider(2.0, 4.5, value=3.5, step=0.1, label=\"Sepal Width (cm)\")\n",
        "            pl = gr.Slider(1.0, 7.0, value=1.4, step=0.1, label=\"Petal Length (cm)\")\n",
        "            pw = gr.Slider(0.1, 2.5, value=0.2, step=0.1, label=\"Petal Width (cm)\")\n",
        "            btn = gr.Button(\"Classify\", variant=\"primary\")\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [5.1, 3.5, 1.4, 0.2],  # Setosa\n",
        "                    [6.0, 3.0, 4.0, 1.2],  # Versicolor\n",
        "                    [7.0, 3.2, 6.0, 2.0]   # Virginica\n",
        "                ],\n",
        "                inputs=[sl, sw, pl, pw],\n",
        "                label=\"Example Measurements\"\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            species = gr.Textbox(label=\"Predicted Species\")\n",
        "            confidence = gr.Textbox(label=\"Confidence\")\n",
        "            probabilities = gr.Textbox(label=\"Class Probabilities\", lines=4)\n",
        "            plot = gr.Plot(label=\"Visualization\")\n",
        "\n",
        "    btn.click(\n",
        "        classify_flower,\n",
        "        inputs=[sl, sw, pl, pw],\n",
        "        outputs=[species, confidence, probabilities, plot]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "xkb-TKcc2R_T",
        "outputId": "c57fd3b8-d02e-477a-b29d-33e14bd93f1e"
      },
      "id": "xkb-TKcc2R_T",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://567efc98b8eb496883.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://567efc98b8eb496883.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Configuration\n",
        "API_URL = \"http://127.0.0.1:8000/predict\"  # Using 127.0.0.1 instead of localhost for better reliability\n",
        "TIMEOUT = 5  # seconds for API requests\n",
        "iris = load_iris()\n",
        "\n",
        "def safe_api_call(url, json_data=None):\n",
        "    \"\"\"Enhanced API call with comprehensive error handling\"\"\"\n",
        "    try:\n",
        "        if json_data:\n",
        "            response = requests.post(url, json=json_data, timeout=TIMEOUT)\n",
        "        else:\n",
        "            response = requests.get(url, timeout=TIMEOUT)\n",
        "\n",
        "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "        return response.json()\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_msg = f\"API Error: {str(e)}\"\n",
        "        if isinstance(e, requests.exceptions.ConnectionError):\n",
        "            error_msg = f\"Could not connect to API at {url}. Please ensure the server is running.\"\n",
        "        elif isinstance(e, requests.exceptions.Timeout):\n",
        "            error_msg = \"API request timed out. The server may be overloaded.\"\n",
        "        return {\"error\": error_msg}\n",
        "\n",
        "def create_probability_plot(probabilities):\n",
        "    \"\"\"Create styled probability bar chart\"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
        "    bars = plt.bar(probabilities.keys(), probabilities.values(), color=colors)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1%}',\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Class Probabilities', fontweight='bold')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return buf\n",
        "\n",
        "def create_feature_comparison(input_features, predicted_species):\n",
        "    \"\"\"Create radar chart comparing input to species averages\"\"\"\n",
        "    # Prepare data\n",
        "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "    df['species'] = iris.target_names[iris.target]\n",
        "    species_avg = df.groupby('species').mean().reset_index()\n",
        "\n",
        "    categories = list(input_features.keys())\n",
        "    N = len(categories)\n",
        "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "    angles += angles[:1]\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    # Plot species averages\n",
        "    for species in species_avg['species']:\n",
        "        values = species_avg[species_avg['species'] == species][categories].values[0].tolist()\n",
        "        values += values[:1]\n",
        "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=species)\n",
        "        ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "    # Plot input features\n",
        "    input_values = [input_features[k] for k in categories] + [input_features[categories[0]]]\n",
        "    ax.plot(angles, input_values, color='red', linewidth=3, linestyle='solid', label='Your Input')\n",
        "    ax.scatter(angles, input_values, color='red', s=100, zorder=10)\n",
        "\n",
        "    # Customize plot\n",
        "    plt.xticks(angles[:-1], categories)\n",
        "    plt.title(f'Feature Comparison (Predicted: {predicted_species})', fontweight='bold')\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return buf\n",
        "\n",
        "def classify_flower(sepal_length, sepal_width, petal_length, petal_width):\n",
        "    \"\"\"Main classification function with visualization\"\"\"\n",
        "    data = {\n",
        "        \"sepal_length\": float(sepal_length),\n",
        "        \"sepal_width\": float(sepal_width),\n",
        "        \"petal_length\": float(petal_length),\n",
        "        \"petal_width\": float(petal_width)\n",
        "    }\n",
        "\n",
        "    # API call with error handling\n",
        "    result = safe_api_call(API_URL, data)\n",
        "\n",
        "    if \"error\" in result:\n",
        "        return result[\"error\"], \"\", \"\", None, None\n",
        "\n",
        "    # Format results\n",
        "    probs = \"\\n\".join([f\"{k}: {v:.1%}\" for k, v in result['probabilities'].items()])\n",
        "\n",
        "    return (\n",
        "        f\"Predicted: {result['species'].upper()}\",\n",
        "        f\"Confidence: {result['confidence']:.1%}\",\n",
        "        probs,\n",
        "        create_probability_plot(result['probabilities']),\n",
        "        create_feature_comparison(data, result['species'])\n",
        "    )\n",
        "\n",
        "# Main Gradio Interface\n",
        "with gr.Blocks(title=\"Iris Classifier\", theme=gr.themes.Soft()) as app:\n",
        "    gr.Markdown(\"# 🌸 Iris Flower Classification Dashboard\")\n",
        "    gr.Markdown(\"Analyze iris measurements and classify species with visual insights\")\n",
        "\n",
        "    with gr.Tab(\"Classifier\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Input Features\")\n",
        "                sl = gr.Slider(4.0, 8.0, value=5.1, label=\"Sepal Length (cm)\", step=0.1)\n",
        "                sw = gr.Slider(2.0, 4.5, value=3.5, label=\"Sepal Width (cm)\", step=0.1)\n",
        "                pl = gr.Slider(1.0, 7.0, value=1.4, label=\"Petal Length (cm)\", step=0.1)\n",
        "                pw = gr.Slider(0.1, 2.5, value=0.2, label=\"Petal Width (cm)\", step=0.1)\n",
        "                btn = gr.Button(\"Classify\", variant=\"primary\")\n",
        "\n",
        "                gr.Markdown(\"### Example Measurements\")\n",
        "                gr.Examples(\n",
        "                    examples=[\n",
        "                        [5.1, 3.5, 1.4, 0.2],  # Setosa\n",
        "                        [6.0, 3.0, 4.0, 1.2],  # Versicolor\n",
        "                        [7.0, 3.2, 6.0, 2.0]   # Virginica\n",
        "                    ],\n",
        "                    inputs=[sl, sw, pl, pw],\n",
        "                    label=\"Try these examples\"\n",
        "                )\n",
        "\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Prediction Results\")\n",
        "                species = gr.Textbox(label=\"Species Prediction\", interactive=False)\n",
        "                confidence = gr.Textbox(label=\"Confidence Score\", interactive=False)\n",
        "                probabilities = gr.Textbox(label=\"Class Probabilities\", lines=4, interactive=False)\n",
        "\n",
        "                gr.Markdown(\"### Probability Visualization\")\n",
        "                prob_plot = gr.Plot(label=\"Class Probabilities\")\n",
        "\n",
        "                gr.Markdown(\"### Feature Comparison\")\n",
        "                feature_plot = gr.Plot(label=\"Feature Radar Chart\")\n",
        "\n",
        "        btn.click(\n",
        "            fn=classify_flower,\n",
        "            inputs=[sl, sw, pl, pw],\n",
        "            outputs=[species, confidence, probabilities, prob_plot, feature_plot]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Data Insights\"):\n",
        "        gr.Markdown(\"## Iris Dataset Visualizations\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Feature Distributions\")\n",
        "                feature_dist_plot = gr.Plot()\n",
        "\n",
        "                gr.Markdown(\"### Decision Boundaries\")\n",
        "                decision_plot = gr.Plot()\n",
        "\n",
        "        # Load visualizations when tab is opened\n",
        "        def load_insights():\n",
        "            # Create feature distribution plot\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(data=pd.DataFrame(iris.data, columns=iris.feature_names).melt(),\n",
        "                       x='variable', y='value', hue=iris.target_names[iris.target])\n",
        "            plt.title(\"Feature Distributions by Species\")\n",
        "            plt.ylabel(\"Measurement (cm)\")\n",
        "            plt.xlabel(\"Feature\")\n",
        "            plt.legend(title=\"Species\")\n",
        "            plt.tight_layout()\n",
        "            dist_buf = BytesIO()\n",
        "            plt.savefig(dist_buf, format='png')\n",
        "            plt.close()\n",
        "\n",
        "            # Create decision boundary plot (simplified)\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            from sklearn.decomposition import PCA\n",
        "            pca = PCA(n_components=2)\n",
        "            X_pca = pca.fit_transform(iris.data)\n",
        "            plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, cmap='viridis')\n",
        "            plt.title(\"PCA Projection of Iris Data\")\n",
        "            plt.xlabel(\"Principal Component 1\")\n",
        "            plt.ylabel(\"Principal Component 2\")\n",
        "            plt.colorbar(label=\"Species\")\n",
        "            decision_buf = BytesIO()\n",
        "            plt.savefig(decision_buf, format='png')\n",
        "            plt.close()\n",
        "\n",
        "            return dist_buf, decision_buf\n",
        "\n",
        "        app.load(load_insights, inputs=None, outputs=[feature_dist_plot, decision_plot])\n",
        "\n",
        "    with gr.Tab(\"Model Info\"):\n",
        "        gr.Markdown(\"## Model Information\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Feature Importances\")\n",
        "                feature_importance = gr.Plot()\n",
        "\n",
        "                # Create feature importance plot\n",
        "                def create_feature_importance():\n",
        "                    features = iris.feature_names\n",
        "                    importances = [0.1, 0.3, 0.4, 0.2]  # Example values\n",
        "\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    plt.barh(features, importances, color='skyblue')\n",
        "                    plt.title(\"Feature Importances\")\n",
        "                    plt.xlabel(\"Importance Score\")\n",
        "                    plt.tight_layout()\n",
        "\n",
        "                    buf = BytesIO()\n",
        "                    plt.savefig(buf, format='png')\n",
        "                    plt.close()\n",
        "                    return buf\n",
        "\n",
        "                feature_importance.value = create_feature_importance()\n",
        "\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### Model Metadata\")\n",
        "                model_type = gr.Textbox(label=\"Model Type\", value=\"Random Forest\", interactive=False)\n",
        "                num_classes = gr.Textbox(label=\"Number of Classes\", value=str(len(iris.target_names)), interactive=False)\n",
        "                num_features = gr.Textbox(label=\"Number of Features\", value=str(len(iris.feature_names)), interactive=False)\n",
        "                training_size = gr.Textbox(label=\"Training Samples\", value=str(len(iris.data)), interactive=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Verify API connection before launching\n",
        "    test_response = safe_api_call(API_URL.replace('/predict', ''))\n",
        "    if \"error\" in test_response:\n",
        "        print(f\"⚠️ {test_response['error']}\")\n",
        "        print(\"\\nPlease start the FastAPI server first with:\")\n",
        "        print(\"uvicorn api.main:app --reload --port 8000\")\n",
        "        print(\"\\nThen run this Gradio interface in a separate terminal.\")\n",
        "    else:\n",
        "        print(\"API connection successful! Launching Gradio interface...\")\n",
        "        app.launch(\n",
        "            server_port=7860,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            show_error=True,\n",
        "            share=False  # Set to True if you want a public link (for Colab)\n",
        "        )\n",
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(8000)  # For API\n",
        "output.serve_kernel_port_as_window(7860)  # For Gradio\n",
        "app.launch(share=True)  # In the Gradio launch command"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "opPjcALl1Ox1",
        "outputId": "76aac78d-d017-45bd-8c21-ae7c45592856"
      },
      "id": "opPjcALl1Ox1",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Could not connect to API at http://127.0.0.1:8000. Please ensure the server is running.\n",
            "\n",
            "Please start the FastAPI server first with:\n",
            "uvicorn api.main:app --reload --port 8000\n",
            "\n",
            "Then run this Gradio interface in a separate terminal.\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8000, \"/\", \"https://localhost:8000/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(7860, \"/\", \"https://localhost:7860/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a235f901fa69d9fd39.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a235f901fa69d9fd39.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['species'] = iris.target_names[iris.target]\n",
        "\n",
        "# 1. Feature Distribution Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=df.melt(id_vars='species'), x='variable', y='value', hue='species')\n",
        "plt.title('Feature Distributions by Species')\n",
        "plt.ylabel('Measurement (cm)')\n",
        "plt.xlabel('Feature')\n",
        "plt.legend(title='Species')\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/feature_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# 2. Pair Plot\n",
        "sns.pairplot(df, hue='species', diag_kind='hist', markers=['o', 's', 'D'])\n",
        "plt.suptitle('Feature Relationships', y=1.02)\n",
        "plt.savefig('outputs/pair_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# 3. Correlation Heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr = df.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/correlation_heatmap.png')\n",
        "plt.close()\n",
        "\n",
        "# 4. Decision Boundaries (PCA Reduced)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(iris.data)\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_pca, iris.target)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_decision_regions(X_pca, iris.target, clf=clf, legend=2)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Decision Boundaries (PCA Reduced)')\n",
        "plt.savefig('outputs/decision_boundaries.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Visualizations saved to current directory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBq5tgcDqRhp",
        "outputId": "74d6bd3f-3583-4859-93b9-21d3e136b189"
      },
      "id": "jBq5tgcDqRhp",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizations saved to current directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a new folder named 'my_folder' in the current directory\n",
        "os.makedirs('.github', exist_ok=True)"
      ],
      "metadata": {
        "id": "6n6Yp8X-sU8R"
      },
      "id": "6n6Yp8X-sU8R",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r colab_files_backup.zip /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY322ebCuhjC",
        "outputId": "b70d9031-0dc0-4bc1-8761-cd627d8c9d90"
      },
      "id": "qY322ebCuhjC",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 22%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2025.06.12/ (stored 0%)\n",
            "  adding: content/.config/logs/2025.06.12/13.35.58.871934.log (deflated 57%)\n",
            "  adding: content/.config/logs/2025.06.12/13.35.39.784909.log (deflated 58%)\n",
            "  adding: content/.config/logs/2025.06.12/13.35.48.692010.log (deflated 86%)\n",
            "  adding: content/.config/logs/2025.06.12/13.35.49.978706.log (deflated 58%)\n",
            "  adding: content/.config/logs/2025.06.12/13.35.19.246604.log (deflated 93%)\n",
            "  adding: content/.config/logs/2025.06.12/13.35.59.537250.log (deflated 57%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "  adding: content/Untitled Folder/ (stored 0%)\n",
            "  adding: content/Untitled Folder/.github/ (stored 0%)\n",
            "  adding: content/Untitled Folder/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/Untitled Folder/untitled (stored 0%)\n",
            "  adding: content/data/ (stored 0%)\n",
            "  adding: content/data/iris_raw.csv (deflated 79%)\n",
            "  adding: content/data/X_train.csv (deflated 67%)\n",
            "  adding: content/data/y_train.csv (deflated 69%)\n",
            "  adding: content/data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/data/X_test.csv (deflated 56%)\n",
            "  adding: content/data/y_test.csv (deflated 54%)\n",
            "  adding: content/pair_plot.png (deflated 3%)\n",
            "  adding: content/monitoring/ (stored 0%)\n",
            "  adding: content/monitoring/prometheus.yml (deflated 39%)\n",
            "  adding: content/monitoring/cm_svm.png (deflated 17%)\n",
            "  adding: content/monitoring/cm_logistic_regression.png (deflated 17%)\n",
            "  adding: content/monitoring/cm_random_forest.png (deflated 18%)\n",
            "  adding: content/.gradio/ (stored 0%)\n",
            "  adding: content/.gradio/certificate.pem (deflated 24%)\n",
            "  adding: content/iris_model.joblib (deflated 87%)\n",
            "  adding: content/correlation_heatmap.png (deflated 14%)\n",
            "  adding: content/decision_boundaries.png (deflated 9%)\n",
            "  adding: content/mlruns/ (stored 0%)\n",
            "  adding: content/mlruns/.trash/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/metrics/accuracy (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/metrics/f1_score (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/meta.yaml (deflated 39%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/outputs/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/outputs/m-881c86506c564aa2a957c4437ab4cdd9/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/outputs/m-881c86506c564aa2a957c4437ab4cdd9/meta.yaml (deflated 36%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/02bcfd2f006648e991bb8cf920ff8455/artifacts/cm_svm.png (deflated 17%)\n",
            "  adding: content/mlruns/655008300926127301/meta.yaml (deflated 30%)\n",
            "  adding: content/mlruns/655008300926127301/models/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/metrics/accuracy (deflated 8%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/metrics/f1_score (deflated 6%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/meta.yaml (deflated 41%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/artifacts/conda.yaml (deflated 34%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/artifacts/MLmodel (deflated 48%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/artifacts/model.pkl (deflated 33%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/artifacts/requirements.txt (deflated 20%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dc21ce9a2d684b13bceecece01574d95/artifacts/python_env.yaml (deflated 16%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/metrics/accuracy (deflated 2%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/metrics/f1_score (deflated 2%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/meta.yaml (deflated 41%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/artifacts/conda.yaml (deflated 34%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/artifacts/MLmodel (deflated 47%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/artifacts/model.pkl (deflated 65%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/artifacts/requirements.txt (deflated 20%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-881c86506c564aa2a957c4437ab4cdd9/artifacts/python_env.yaml (deflated 16%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/metrics/accuracy (deflated 4%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/metrics/f1_score (deflated 4%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/meta.yaml (deflated 41%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/artifacts/conda.yaml (deflated 34%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/artifacts/MLmodel (deflated 47%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/artifacts/model.pkl (deflated 87%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/artifacts/requirements.txt (deflated 20%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-bfa13ba755d34080afbc1b49cea836de/artifacts/python_env.yaml (deflated 16%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/metrics/accuracy (deflated 2%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/metrics/f1_score (deflated 2%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/meta.yaml (deflated 41%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/artifacts/conda.yaml (deflated 34%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/artifacts/MLmodel (deflated 47%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/artifacts/model.pkl (deflated 33%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/artifacts/requirements.txt (deflated 20%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-9ebc73d55674408097573767ca7b8c64/artifacts/python_env.yaml (deflated 16%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/metrics/accuracy (deflated 2%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/metrics/f1_score (deflated 4%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/meta.yaml (deflated 41%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/artifacts/conda.yaml (deflated 34%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/artifacts/MLmodel (deflated 48%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/artifacts/model.pkl (deflated 87%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/artifacts/requirements.txt (deflated 20%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-dea5154b647b4290afece9e0de2d46e0/artifacts/python_env.yaml (deflated 16%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/metrics/accuracy (deflated 4%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/metrics/f1_score (deflated 4%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/meta.yaml (deflated 41%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/artifacts/conda.yaml (deflated 34%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/artifacts/MLmodel (deflated 47%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/artifacts/model.pkl (deflated 65%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/artifacts/requirements.txt (deflated 20%)\n",
            "  adding: content/mlruns/655008300926127301/models/m-40ba706ee0594f82b568ea241e24212f/artifacts/python_env.yaml (deflated 16%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/metrics/accuracy (deflated 43%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/metrics/f1_score (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/meta.yaml (deflated 39%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/outputs/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/outputs/m-40ba706ee0594f82b568ea241e24212f/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/outputs/m-40ba706ee0594f82b568ea241e24212f/meta.yaml (deflated 36%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/7af705d3d8dd4963a59af661c7a7473e/artifacts/cm_svm.png (deflated 17%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/metrics/accuracy (deflated 43%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/metrics/f1_score (deflated 43%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/meta.yaml (deflated 40%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/outputs/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/outputs/m-bfa13ba755d34080afbc1b49cea836de/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/outputs/m-bfa13ba755d34080afbc1b49cea836de/meta.yaml (deflated 36%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/47a848b640f44cb6b9e6ac9866e004a8/artifacts/cm_random_forest.png (deflated 18%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/metrics/accuracy (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/metrics/f1_score (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/meta.yaml (deflated 40%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/outputs/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/outputs/m-dea5154b647b4290afece9e0de2d46e0/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/outputs/m-dea5154b647b4290afece9e0de2d46e0/meta.yaml (deflated 36%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/a74ea3ad2a054addb967c8dbf7e42215/artifacts/cm_random_forest.png (deflated 18%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/metrics/accuracy (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/metrics/f1_score (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/meta.yaml (deflated 40%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/outputs/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/outputs/m-9ebc73d55674408097573767ca7b8c64/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/outputs/m-9ebc73d55674408097573767ca7b8c64/meta.yaml (deflated 36%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/064d792a000f40edad55a9dcb346a3f6/artifacts/cm_logistic_regression.png (deflated 17%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/metrics/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/metrics/accuracy (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/metrics/f1_score (deflated 38%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/meta.yaml (deflated 40%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/params/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/params/model (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/outputs/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/outputs/m-dc21ce9a2d684b13bceecece01574d95/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/outputs/m-dc21ce9a2d684b13bceecece01574d95/meta.yaml (deflated 37%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/tags/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/tags/mlflow.source.name (deflated 5%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/tags/mlflow.runName (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/tags/mlflow.user (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/tags/mlflow.source.type (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/artifacts/ (stored 0%)\n",
            "  adding: content/mlruns/655008300926127301/94102186f16441e4b31349332486433a/artifacts/cm_logistic_regression.png (deflated 17%)\n",
            "  adding: content/mlruns/0/ (stored 0%)\n",
            "  adding: content/mlruns/0/meta.yaml (deflated 24%)\n",
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/iris_classifier.joblib (deflated 40%)\n",
            "  adding: content/models/iris_pipeline.joblib (deflated 40%)\n",
            "  adding: content/outputs/ (stored 0%)\n",
            "  adding: content/outputs/pair_plot.png (deflated 3%)\n",
            "  adding: content/outputs/correlation_heatmap.png (deflated 14%)\n",
            "  adding: content/outputs/decision_boundaries.png (deflated 9%)\n",
            "  adding: content/outputs/feature_distribution.png (deflated 22%)\n",
            "  adding: content/.github/ (stored 0%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/README.md (deflated 39%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/iris_pipeline.joblib (deflated 40%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/api/ (stored 0%)\n",
            "  adding: content/api/main.py (deflated 61%)\n",
            "  adding: content/api/Dockerfile (deflated 33%)\n",
            "  adding: content/api/requirements.txt (stored 0%)\n",
            "  adding: content/api/__pycache__/ (stored 0%)\n",
            "  adding: content/api/__pycache__/main.cpython-311.pyc (deflated 47%)\n",
            "  adding: content/feature_distribution.png (deflated 22%)\n",
            "  adding: content/tests/ (stored 0%)\n",
            "  adding: content/tests/test_api.py (deflated 62%)\n",
            "  adding: content/github/ (stored 0%)\n",
            "  adding: content/github/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/github/workflows/ (stored 0%)\n",
            "  adding: content/github/workflows/ml-pipeline.yml (deflated 57%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb36cfb",
      "metadata": {
        "id": "4cb36cfb"
      },
      "source": [
        "## 10. Future Enhancements\n",
        "\n",
        "1. **Data Versioning**: Use DVC for data version control\n",
        "2. **Feature Store**: Implement a feature store for reusable features\n",
        "3. **Model Registry**: Use MLflow Model Registry for versioning\n",
        "4. **Drift Detection**: Monitor data and concept drift\n",
        "5. **A/B Testing**: Implement canary deployments\n",
        "6. **AutoML**: Integrate automated model selection\n",
        "7. **Scalability**: Add Kubernetes for orchestration\n",
        "8. **Security**: Implement authentication and rate limiting\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Data Collection] --> B[Preprocessing]\n",
        "    B --> C[Training]\n",
        "    C --> D[Validation]\n",
        "    D --> E[Deployment]\n",
        "    E --> F[Monitoring]\n",
        "    F --> G[Retraining]\n",
        "    G --> C\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}