# ğŸ® Day 27: Reinforcement Learning with CartPole

## ğŸ§  Project Title
**Deep Q-Learning Agent for CartPole-v1 (OpenAI Gym)**

---

## ğŸ” Overview

This project demonstrates how to train a reinforcement learning agent using **Deep Q-Learning (DQN)** to solve the classic **CartPole-v1** environment from OpenAI Gym.

In the CartPole task, the agent learns to balance a pole on a moving cart using discrete actions (move left or right) based on the state of the environment.

---

## ğŸ—‚ï¸ Directory Structure

Day27_Reinforcement_Learning_CartPole/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ rewards_log.csv # Episode rewards log (optional)
â”‚
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ training_rewards.png # Plot of episode rewards over time
â”‚ â””â”€â”€ cartpole_agent.gif # Optional animated visualization
â”‚
â”œâ”€â”€ videos/
â”‚ â”œâ”€â”€ cartpole_agent.mp4 # Trained agent's gameplay video
â”‚ â”œâ”€â”€ rl-video-episode-0.mp4 # Episode 0 video (sample)
â”‚ â””â”€â”€ rl-video-episode-0.meta.json # Metadata
â”‚
â”œâ”€â”€ artifacts/
â”‚ â”œâ”€â”€ cartpole_dqn_episode_0.h5 # Weights after episode 0
â”‚ â”œâ”€â”€ cartpole_dqn_episode_0_target.h5 # Target network after episode 0
â”‚ â”œâ”€â”€ cartpole_dqn_episode_50.h5 # Mid-training weights
â”‚ â”œâ”€â”€ cartpole_dqn_episode_50_target.h5 # Mid-training target
â”‚ â”œâ”€â”€ cartpole_dqn_final.h5 # Final trained model
â”‚ â””â”€â”€ cartpole_dqn_final_target.h5 # Final target model
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ Day_27_Reinforcement_Learning_CartPole.ipynb # Main training and visualization notebook
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ environment.py # OpenAI Gym environment setup
â”‚ â”œâ”€â”€ agent.py # DQN Agent logic
â”‚ â”œâ”€â”€ train.py # Training loop
â”‚ â””â”€â”€ utils.py # Plotting, saving models, etc.
â”‚
â”œâ”€â”€ requirements.txt # Required Python packages
â”œâ”€â”€ .gitignore # Ignore saved weights, logs, etc.
â””â”€â”€ README.md # This file

yaml
Copy
Edit

---

## ğŸ¯ Learning Outcomes

âœ… Understand the basics of reinforcement learning  
âœ… Implement a Deep Q-Network (DQN) using PyTorch  
âœ… Apply epsilon-greedy exploration strategy  
âœ… Use experience replay buffers for training stability  
âœ… Visualize training curves and agent performance  
âœ… Save trained models and record gameplay

---

## ğŸ“ˆ Visualizations

- `training_rewards.png` â€” Plot showing episode rewards over time  
- `cartpole_agent.mp4` â€” Recorded video of trained agent's performance

---

## ğŸ§ª How to Run

### 1. Install Dependencies
```bash
pip install -r requirements.txt
2. Run the Notebook
bash
Copy
Edit
jupyter notebook notebooks/Day_27_Reinforcement_Learning_CartPole.ipynb
3. (Optional) Run the Python Script
bash
Copy
Edit
python src/train.py
ğŸ§  Model Architecture: Deep Q-Network (DQN)
scss
Copy
Edit
Input (State Vector)
      â¬‡ï¸
Fully Connected Layer (128 units, ReLU)
      â¬‡ï¸
Fully Connected Layer (64 units, ReLU)
      â¬‡ï¸
Output Layer (2 Q-values: Left, Right)
ğŸ§  Agent Components
Experience Replay: Stores transitions and samples mini-batches

Target Network: Stabilizes learning

Epsilon-Greedy Strategy: Balances exploration vs. exploitation

Huber Loss: Handles outlier Q-values

Model Saving: Periodic weight checkpoints

ğŸ“Œ Stay Tuned
ğŸš€ This is Day 27 of my #30DaysMLProjects journey.
